<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Anming Gu</title>
  
  <meta name="author" content="Anming Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Anming Gu</name>
              </p>
              <p>I'm a recent graduate of <a href="https://www.bu.edu/">Boston University</a>. I am currently working with <a href="https://cs-people.bu.edu/edchien/">Prof. Edward Chien</a> on applied optimal transport in statistics and machine learning.
              </p>
              <p>
                My graduate coursework includes:
                <ul>
                  <li>Mathematics: Functional Analysis, Stochastic Calculus, Mathematics of Deep Learning, PDEs, Stochastic PDEs</li>
                  <li>Computer Science: Complexity Theory, Mathematical Methods for Theoretical Computer Science</li>
                </ul>
              </p>

              <p>
                Teaching experience: 
                <ul>
                  <li>Theory of Computation, Spring 2024</li>
                  <li>Concepts of Programming Languages, Fall 2023</li>
                  <li>Analysis of Algorithms, Spring 2022</li>
                </ul>
              </p>
              <p style="text-align:center">
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=AVjp1ykAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/AnmingGu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.jpeg">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpeg" class="hoverZoomLink">
              </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in optimal transport, stochastic calculus, machine learning theory, and theoretical computer science.
              </p>
              <p>
                Some problems I'm interested in recently: 
                <ul>
                  <li>Using stochastic analysis and Wasserstein gradient flows for Boolean Fourier analysis</li>
                  <li>Estimating Wasserstein distance from empirical samples: beating the curse of dimensionality, using non-empirical measures</li>
                  <li>Distribution testing in Wasserstein distance</li>
                  <li>Wasserstein distance bounds on the evolution of SDEs, e.g. Langevin equations in Euclidean space, Brownian motion on Riemannian manifolds</li>
                  <li>Johnson-Lindenstrauss type results for compressing shallow neural networks</li>
                  <li>Topological and metric results on convergence of Monge maps in regularized optimal transport</li>
                  <li>Optimal transport for discrete objects (graphs, spin glasses), generalization of LLMs, and differential privacy</li>
                  <li>Optimal transport for stochastic continuity equation</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/traj_inference.png" alt="traj_inference" width="160" height="140">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior</papertitle>
              </a>
              <br>
              Anming Gu, Edward Chien, Kristjan Greenewald
              <br>
              Pending submission to NeurIPS, 2024.
              <br>
              link to come / code to come / <a href="data/thesis_slides.pdf">thesis slides</a>
              <br>
              <!-- <a href="https://arxiv.org/abs/2106.02933">arXiv</a> / <a href="https://github.com/AnmingGu/kmixup-cifar10">code</a> -->
              <p>
                Trajectory inference is the problem of recovering a stochastic process from temporal marginals. We consider the setting when we cannot observe the process directly but we have access to a known velocity field. Using tools in optimal transport, stochastic calculus, and optimization theory, we show that a minimum entropy estimator will recover the latent trajectory of the process. We provide theoretical guarantees that our estimator will converge to the ground truth as the number of observations becomes dense in the time domain. We also provide empirical results to show the robustness of our method.
              </p>
            </td>
          <tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scaling_checkpoint.png" alt="scaling_checkpoint" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>Larger Language Models Provably Generalize Better</papertitle>
              </a>
              <br>
              Marc Anton Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Andrew Gordon Wilson, J Zico Kolter, Christopher De Sa
              <br>
              Pending submission to NeurIPS, 2024.
              <br>
              link to come / code to come
              <br>
              <p>
                Why do larger language models generalize better? To address this question, we develop generalization bounds on the LLM pretraining objective in the compute optimal regime. We prove a novel fully empirical Freedman-type martingale concentration inequality, tightening existing bounds to account for the low loss variance. With larger models this variance decreases, meaning that our generalization bounds can even get tighter as the models get larger. We pair these findings with an analysis of the theoretically achievable quantization bitrates based on the Hessian of the loss function, controlling the other component of the bounded gap. With these results, we move towards a more complete understanding of why LLMs generalize.
              </p>
            </td>
          <tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kmixup.png" alt="kmixup" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.02933">
                <papertitle>k-Mixup Regularization for Deep Learning via Optimal Transport</papertitle>
              </a>
              <br>
              Kristjan Greenewald, Anming Gu, Mikhail Yurochkin, Justin Solomon, Edward Chien
              <br>
              Transactions on Machine Learning Research, 2023.
              <br>
              <a href="https://arxiv.org/abs/2106.02933">arXiv</a> / <a href="https://github.com/AnmingGu/kmixup-cifar10">code</a>
              <p>
                Mixup is a regularization technique for training neural networks that perturbs input training data in the direction of other randomly chosen training data. We propose a new variant of mixup that uses optimal transport to perturb training data in the direction of other training data that are more similar to the input data. We show theoretically and experimentally that our method is more effective than mixup at improving generalization performance.
              </p>
            </td>
          </tr>
          
        </tbody></table>

        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Course Notes</heading>
            <p>
              Inspired by <a href="https://www.ekzhang.com/">Eric Zhang</a>
            </p>
            <ul>
              <li><a href="notes/functional_analysis.pdf">Functional Analysis</a></li>
              <li><a href="notes/financial_econometrics.pdf">Financial Econometrics</a></li>
            </ul>
          </td>
        </tr>
        </tbody></table>
      </tbody></table>
      <table style="width:50%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              Template from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">webpage</a>
            </p>
          </td>
        </tr>
      </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
